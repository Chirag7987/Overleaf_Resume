\documentclass{article}

\usepackage[top=0.2in, bottom=0.3in, left=0.3in, right=0.3in]{geometry}
\usepackage{enumitem}
\usepackage{hyperref} % Include the hyperref package


\begin{document}
\begin{center}
\thispagestyle{empty}
\large \textbf{Amisha Garg \\}
\normalsize New York, United States $\mid$ +1(229)4626192 $\mid$ \href{mailto:amishagarg828@gmail.com}{Email} $\mid$ \href{https://www.linkedin.com/in/your-linkedin}{LinkedIn} $\mid$ \href{https://github.com/your-github}{GitHub} \\
\rule{\textwidth}{1pt}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EDUCATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{\underline{EDUCATION}} \\
\textbf{Master of Science in Data Science} GPA: 3.83  \hfill \textbf{August 2022 - May 2024} \\
\textit{Clarkon University , Potsdam, NY, USA}\\
\textbf{Coursework}: Data Mining, Data Warehousing, Machine Learning, Visualization of Complex Data, Cloud Computing, BigData\\

\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\end{itemize}
\textbf{Bachelor of Technology in Computer Science and Engineering} GPA: 8.2 \hfill \textbf{August 2018 - May 2022} \\
\textit{Mody University , India} \\
\textbf{Coursework}: DBMS, Operating Systems, Data Structures and Algorithms, OOP'S, Computer Architecture.

\vspace{3mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TECHNICAL PROFICIENCY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{\underline{TECHNICAL PROFICIENCY}} \\
\textbf{Programming Languages}{: \small Python (Pandas, NumPy, Matplotlib, Seaborn), R, SQL, Java, Scala} \\
\textbf{Data Processing}{: \small Apache Spark, Apache Airflow, Apache Kafka for real-time data streaming, Hadoop} \\
\textbf{Cloud Platforms}{: \small AWS (S3, EMR, RDS, Glue, Lambda, Redshift), Azure (Data Lake, HDInsight, Databricks), Dataflow} \\
\textbf{Databases and Data Warehousing}{: \small MySQL, PostgreSQL, MongoDB, Redis, Snowflake, Amazon Redshift} \\
\textbf{Data Visualization Tools}{: \small Prometheus, Grafana, ELK Stack, AWS CloudWatch, Tableau, Power BI, Apache Superset} \\
\textbf{Version Control and CI/CD}{: \small Git, GitHub, GitLab, Jenkins, GitHub Actions, Bitbucket, CircleCI, Docker, Kubernetes } \\
\textbf{Monitoring and Logging}{: \small Prometheus, Grafana, ELK Stack, AWS CloudWatch} \\
\textbf{Data Modeling and Design}{: \small ERD, Dimensional Modeling, Star Schema, Snowflake Schema} 
\vspace{3mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WORK EXPERIENCE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{\underline{WORK EXPERIENCE}} \\
\noindent \textbf{Data Engineer $\mid$ Telos Air} \hfill \textbf{Februry 2024 – Present}
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item {\small Engineered scalable data pipelines using AWS services (EC2, S3, Lambda, Glue), reducing data processing time by 40\% and improving storage efficiency by 30\%. Designed automated workflows to handle large volumes of data with minimal latency.}
\item {\small Implemented Apache Kafka for real-time data streaming, increasing IoT application performance by 25\% and boosting data throughput by 50\%. Integrated and processed data from various IoT devices in real-time.}
\item {\small Developed robust ETL processes using Apache Airflow, automating data integration and ensuring 99.9\% data consistency across 20+ disparate data sources. Scheduled, monitored, and managed complex data workflows.}
\item {\small Leveraged Apache Spark on Databricks within Azure for big data processing, accelerating analytics tasks by 60\% and facilitating efficient machine learning model training. Handled massive datasets for quicker insights.}
\end{itemize}
\vspace{1mm}

\noindent \textbf{Technology Consultant-Graduate Assistant $\mid$ Clarkson University} \hfill \textbf{August 2022 – December 2023}
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item {\small Promoted the use of cloud data warehouses like Snowflake and Amazon Redshift, leading to a 35\% increase in data storage efficiency and analytics performance for students. Guided students on data warehousing best practices.}
\item {\small Conducted over 10 workshops on big data technologies like Hadoop and Spark, providing hands-on experience to 150+ students. Focused on practical applications of data lakes and scalable data processing techniques.}
\item {\small Implemented and optimized data pipelines for student projects using Python and Apache Airflow, improving data processing speeds by 40\% and ensuring data consistency across multiple sources.}
\end{itemize}
\vspace{1mm}

\noindent \textbf{Data Engineer $\mid$ Ericsson} \hfill \textbf{May 2021 – August 2022}
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item {\small Automated data ingestion and transformation pipelines using Talend, resulting in a 40\% increase in data availability and a 25\% improvement in data quality. Created automated workflows to ensure timely and accurate data for business intelligence.}
\item {\small Designed and deployed data models and architectures for operational and analytical purposes, optimizing performance by 30\% and scalability in cloud environments by 50\%. Implemented best practices for data modeling and architecture design.}
\item {\small Implemented CI/CD for data pipeline code using Jenkins and GitLab CI, reducing deployment cycles by 20\% and improving pipeline reliability by 35\%. Automated testing and deployment of data pipelines.}
\end{itemize}





\vspace{3mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PROJECT WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{\underline{PROJECT WORK}}\\
\noindent \textbf{Full Stack Component Application $\mid$ Python, AWS EC2, SSH, Python, Docker, MySQL} \textit{\hfill \textbf{December 2023}}
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item {\small Developed a Python script to automate AWS EC2 setups, reducing provisioning time by 50\% and manual intervention by 75\%. The script generated SSH keys, launched EC2 instances, and installed tools like Python, JupyterLab, Docker, MySQL, and PHPMyAdmin.}
\item {\small Implemented functionality to generate SSH keys, launch EC2 instances, and upload a customizable bash script containing commands to install essential tools and software like Python, JupyterLab, Docker, MySQL, and PHPMyAdmin, cutting setup time by 60\%.}
\end{itemize}
\vspace{1mm}

\noindent \textbf{New York Taxi Data Integration And Analysis $\mid$ Python, SQL} \textit{\hfill \textbf{December 2022}}
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item {\small Optimized New York Taxi data integration and analysis using Python and MySQL; merged datasets totaling 28 million rows, reducing data processing time by 35\% and demonstrating advanced data manipulation skills.}
\item {\small Developed an Entity-Relationship Diagram (ERD) for database optimization and executed it in MySQL, increasing query performance by 40\% and showcasing database architecture and management proficiency.}
\end{itemize}
\vspace{1mm}

\noindent \textbf{Skin Cancer Classification $\mid$ TensorFlow, Python, Matplotlib, AWS} \textit{\hfill \textbf{April 2023}}
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item {\small Engineered an advanced Convolutional Neural Network (CNN) with an attention mechanism, achieving 95\% classification accuracy on the HAM1000 dataset. Leveraged TensorFlow, Python, and Matplotlib for development, visualization, and preprocessing. Enhanced model robustness through strategic oversampling, correcting data imbalance and improving model performance by 20\%.}
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AWARDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
